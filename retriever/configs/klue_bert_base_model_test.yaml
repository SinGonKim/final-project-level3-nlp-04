# TrainingArguments
output_dir: 'output'
seed: 42
evaluation_strategy: 'steps'
per_device_train_batch_size: 16
per_device_eval_batch_size: 16
gradient_accumulation_steps: 2
num_train_epochs: 30
learning_rate: !!float 5e-5
weight_decay: 0.1
adam_epsilon: !!float 1e-8
warmup_ratio: 0.1
logging_steps: 100
fp16: True
save_total_limit: 2
eval_steps: 100
save_steps: 100
early_stopping_patience: 5

# ModelArguments
model_name_or_path: 'klue/bert-base'
config_name: 'klue/bert-base'
tokenizer_name: 'klue/bert-base'

# DataTrainingArguments
dataset_path: '/opt/ml/final_project/data/retrieval_dataset'
overwrite_cache: False
preprocessing_num_workers: None
max_seq_length: 384
pad_to_max_length: False
doc_stride: 128
top_k_retrieval: 30
use_faiss: False

# WandBArguments
project_name: 'final_project_dense_retriever_test'
run_name: 'klue_bert_base'
